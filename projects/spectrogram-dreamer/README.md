# `An√°lise de Sinais de √Åudio com Modelos de Mundo`
# `Audio Signal Analysis with World Models`

## Apresenta√ß√£o

Este projeto foi originado no contexto das atividades da disciplina de p√≥s-gradua√ß√£o *IA376N - IA Generativa: dos modelos √†s aplica√ß√µes multimodais*, oferecida no segundo semestre de 2025, na Unicamp, sob supervis√£o da Profa. Dra. Paula Dornhofer Paro Costa, do Departamento de Engenharia de Computa√ß√£o e Automa√ß√£o (DCA) da Faculdade de Engenharia El√©trica e de Computa√ß√£o (FEEC).

|Nome  | RA | Especializa√ß√£o|
|--|--|--|
| Davi Pincinato  | 157810  | Eng. Computa√ß√£o |
| Henrique Parede de Souza  | 260497  | Eng. Computa√ß√£o|
| Isadora Minuzzi Vieira  | 290184  | Eng. Biom√©dica|
| Raphael Carvalho da Silva e Silva  | 205125  | Eng. Computa√ß√£o |

## Resumo (Abstract)

Este projeto adapta modelos de mundo (World Models) √† s√≠ntese de √°udio utilizando a arquitetura DreamerV2. Implementamos um encoder convolucional para extrair embeddings de espectrogramas log-mel, um RSSM (Recurrent State Space Model) com estados determin√≠sticos (GRU) e estoc√°sticos (gaussianos) para capturar din√¢micas temporais, e um decoder para reconstru√ß√£o. O sistema foi treinado no dataset Common Voice com 652h de √°udio, utilizando perdas de reconstru√ß√£o (MSE) e diverg√™ncia KL. Resultados demonstram reconstru√ß√µes consistentes de espectrogramas e estabilidade no treinamento do espa√ßo latente, evidenciando o potencial de World Models para modelagem temporal de sinais de √°udio. Pr√≥ximos passos incluem integra√ß√£o do m√≥dulo actor-critic e s√≠ntese por imagina√ß√£o.

## Descri√ß√£o do Problema/Motiva√ß√£o

Modelos de mundo (World Models) surgiram no contexto de aprendizado por refor√ßo como forma de aprender representa√ß√µes latentes das din√¢micas do ambiente [HA et al., 2018]. Ao inv√©s de reagir apenas a observa√ß√µes imediatas, um modelo de mundo aprende a prever e "imaginar" futuros estados em seu pr√≥prio espa√ßo latente, permitindo planejamento e aprendizado de pol√≠ticas mais eficientes.

A arquitetura DreamerV2 [HAFNER et al., 2020] se destaca ao combinar um modelo de mundo latente com aprendizado de pol√≠ticas inteiramente neste espa√ßo, dispensando reconstru√ß√£o pixel-a-pixel de observa√ß√µes visuais. Enquanto o DreamerV2 demonstrou sucesso em ambientes visuais complexos como jogos Atari, sua aplica√ß√£o ao dom√≠nio de √°udio permanece inexplorada.

Este projeto prop√µe transportar o conceito de modelos de mundo para o dom√≠nio do √°udio, substituindo imagens por espectrogramas. A motiva√ß√£o reside em explorar capacidades de: (1) previs√£o de sequ√™ncias temporais, (2) completude de padr√µes ac√∫sticos, (3) s√≠ntese condicionada, (4) aprendizado self-supervised de representa√ß√µes e (5) robustez em tarefas de reconhecimento autom√°tico de fala (ASR). A representa√ß√£o em espa√ßo latente permite capturar estruturas e transi√ß√µes temporais complexas inerentes aos sinais sonoros.

## Objetivo

**Objetivo Geral:**
Treinar um modelo de mundo capaz de aprender e prever a evolu√ß√£o temporal de espectrogramas de √°udio em espa√ßo latente, permitindo s√≠ntese por imagina√ß√£o an√°loga ao DreamerV2.

**Objetivos Espec√≠ficos:**
1. Definir pipeline de pr√©-processamento: convers√£o de √°udio para espectrogramas log-mel e divis√£o do dataset
2. Implementar e treinar encoder convolucional para extra√ß√£o de embeddings de espectrogramas
3. Implementar RSSM (Recurrent State Space Model) com estados determin√≠sticos (GRU) e estoc√°sticos (gaussianos)
4. Implementar decoder para reconstru√ß√£o de espectrogramas a partir de estados latentes
5. Treinar modelo de mundo completo (encoder + RSSM + decoder) com perdas de reconstru√ß√£o e KL
6. Avaliar qualidade de reconstru√ß√£o e estabilidade do espa√ßo latente
7. Implementar m√≥dulo actor-critic para s√≠ntese por planejamento no espa√ßo latente (trabalhos futuros)

## Metodologia

### 1. Pr√©-processamento de √Åudio

**Convers√£o para Espectrogramas:**
- Utiliza√ß√£o da Transformada de Fourier de Curto Tempo (STFT) via `torchaudio` para decompor sinais temporais em espectrogramas
- Convers√£o para escala log-mel (80 mel-bands) que aproxima a percep√ß√£o auditiva humana
- Aplica√ß√£o de logaritmo para comprimir faixa din√¢mica, tornando caracter√≠sticas sutis mais vis√≠veis

**Normaliza√ß√£o:**
- Normaliza√ß√£o z-score por amostra para garantir converg√™ncia durante treinamento
- Padroniza√ß√£o de dimens√µes: espectrogramas de forma (1, H, W) onde H=80 (mel-bands) e W varia com dura√ß√£o

**Divis√£o do Dataset:**
- Split de 90% treino / 10% valida√ß√£o
- Filtro de sequ√™ncias com comprimento ‚â• 20 frames para garantir contexto temporal suficiente

**Ferramentas:** `librosa`, `torchaudio`, `torch`, `numpy`

### 2. Arquitetura do Modelo de Mundo

**Encoder Convolucional:**
- CNN com m√∫ltiplas camadas convolucionais (depth=32) seguidas de ativa√ß√µes ELU
- MLP de 2 camadas (hidden_dim=400) que recebe concatena√ß√£o de: features CNN + estado determin√≠stico h_t
- Sa√≠da: embeddings de dimens√£o 256 que alimentam o RSSM
- Baseado na implementa√ß√£o `pydreamer` com adapta√ß√µes para espectrogramas

**RSSM (Recurrent State Space Model):**
N√∫cleo do modelo com tr√™s componentes interconectados:

1. **Modelo Din√¢mico (GRU):**
   - Atualiza estado determin√≠stico: h_t = GRU(h_{t-1}, [z_{t-1}, a_t])
   - Captura mem√≥ria temporal de longo prazo
   - Dimens√£o: h_state_size = 200

2. **Prior (Modelo de Transi√ß√£o):**
   - Prediz pr√≥ximo estado estoc√°stico sem observa√ß√£o: p(z_t | h_t)
   - MLP de 2 camadas ‚Üí distribui√ß√£o gaussiana (Œº, œÉ)
   - Permite imagina√ß√£o/rollouts sem dados reais
   - Dimens√£o: z_state_size = 30

3. **Posterior (Modelo de Representa√ß√£o):**
   - Infere estado estoc√°stico atual com observa√ß√£o: q(z_t | h_t, o_t)
   - MLP de 2 camadas recebendo [h_t, embedding_t] ‚Üí distribui√ß√£o gaussiana
   - Usado durante treinamento para infer√™ncia

**Decoder:**
- MLP de 2 camadas que recebe estado latente completo [h_t, z_t]
- CNN transposta para reconstru√ß√£o do espectrograma
- Sa√≠da: espectrograma reconstru√≠do de mesma dimens√£o que entrada

**Predictores Auxiliares:**
- **Reward Predictor:** MLP que estima "recompensa" de qualidade ac√∫stica a partir de [h_t, z_t]
- **Style Reward Predictor:** MLP para recompensa de consist√™ncia de estilo
- Prepara√ß√£o para futura integra√ß√£o do actor-critic

### 3. Treinamento do Modelo de Mundo

**Fun√ß√£o de Perda:**
```
L_total = L_recon + Œ≤_kl * L_kl + Œ≤_reward * L_reward
```

- **L_recon (MSE):** Erro quadr√°tico m√©dio entre espectrograma original e reconstru√≠do
- **L_kl:** Diverg√™ncia KL entre posterior q(z_t | h_t, o_t) e prior p(z_t | h_t)
  - Regulariza o espa√ßo latente
  - For√ßa prior a aprender predi√ß√µes consistentes sem observa√ß√µes
  - Essencial para rollouts imaginados
- **L_reward:** MSE entre recompensas preditas e calculadas (prepara√ß√£o para RL)

**Hiperpar√¢metros:**
- Batch size: 16 sequ√™ncias
- Sequence length: 20 frames temporais
- Learning rate: 1e-4 (Adam)
- Œ≤_kl: 1.0 (peso da diverg√™ncia KL)
- √âpocas: 100

**Estrat√©gia de Treinamento:**
- Otimizadores separados para world model (encoder + RSSM + decoder) e predictores
- Treinamento conjunto end-to-end
- Valida√ß√£o a cada √©poca para monitorar generaliza√ß√£o
- Checkpoints salvos a cada 10 √©pocas

**Ferramentas:** `PyTorch`, `MLflow` (tracking), `tqdm` (progress)

### 4. Metodologia de Avalia√ß√£o

**M√©tricas Quantitativas:**
- **Perda de Reconstru√ß√£o (MSE):** Avalia fidelidade visual do espectrograma reconstru√≠do
- **Diverg√™ncia KL:** Monitora regulariza√ß√£o do espa√ßo latente
- **PSNR (Peak Signal-to-Noise Ratio):** Qualidade objetiva de reconstru√ß√£o
- **Correla√ß√£o de Pearson:** Similaridade entre distribui√ß√µes espectrais

**An√°lises Qualitativas:**
- Compara√ß√£o visual de espectrogramas originais vs. reconstru√≠dos
- An√°lise de estabilidade durante treinamento (curvas de loss)
- Inspe√ß√£o de trajet√≥rias no espa√ßo latente (prepara√ß√£o para visualiza√ß√£o t-SNE/UMAP)

**Avalia√ß√µes Futuras (E3):**
- Qualidade de rollouts imaginados (gera√ß√£o sem observa√ß√£o)
- Completude de sequ√™ncias parciais
- Coer√™ncia temporal de √°udio sintetizado via Griffin-Lim
- Perplexidade e uso do espa√ßo latente

### Bases de Dados e Evolu√ß√£o

|Base de Dados | Endere√ßo na Web | Resumo descritivo|
|----- | ----- | -----|
|Common Voice Dataset v4 | https://www.kaggle.com/datasets/vedant2022/common-voice-dataset-version-4 | Dataset de fala em ingl√™s validado por crowdsourcing contendo ~889h de grava√ß√µes com transcri√ß√µes, idade, g√™nero e sotaque dos falantes. Diversidade fon√©tica e variabilidade de locutores ideal para aprendizado self-supervised.|

**Caracter√≠sticas do Dataset:**
- **Formato:** Arquivos MP3 de √°udio + metadados CSV
- **Tamanho original:** 889 horas validadas
- **Tamanho p√≥s-filtragem:** 652h33min (sequ√™ncias ‚â• 20 frames)
- **Anota√ß√µes:** Transcri√ß√µes textuais, demografia dos falantes
- **Sample rate:** 48kHz (convertido para 22.05kHz no pr√©-processamento)

**Transforma√ß√µes Realizadas:**
1. Convers√£o para espectrogramas log-mel (n_mels=80, hop_length=512, n_fft=2048)
2. Normaliza√ß√£o z-score por amostra
3. Filtro de comprimento m√≠nimo (‚â•20 frames)
4. Split 90/10 (treino/val)
5. Armazenamento em formato HDF5 para leitura eficiente

**Estat√≠sticas Descritivas:**
- **Total de amostras:** ~200.000 sequ√™ncias
- **Treino:** ~180.000 sequ√™ncias
- **Valida√ß√£o:** ~20.000 sequ√™ncias
- **Dura√ß√£o m√©dia por amostra:** ~11.7 segundos
- **Distribui√ß√£o de locutores:** 2.454 √∫nicos
- **Distribui√ß√£o de g√™nero:** 72% masculino, 26% feminino, 2% outros

### Workflow
<img width="4252" height="1080" alt="workflow" src="https://github.com/user-attachments/assets/cc627853-7df2-4f4c-8766-c368a56a91ef" />

**Legenda do Workflow:**
1. **Pr√©-processamento:** Convers√£o de √°udio para espectrogramas log-mel
2. **Encoder:** Extra√ß√£o de embeddings via CNN + MLP
3. **RSSM:** Modelagem temporal com estados determin√≠sticos (h_t) e estoc√°sticos (z_t)
4. **Decoder:** Reconstru√ß√£o de espectrogramas a partir de estados latentes
5. **P√≥s-processamento:** Convers√£o de espectrograma para √°udio via Griffin-Lim

## Experimentos, Resultados e Discuss√£o dos Resultados

### 1. Configura√ß√£o Experimental

**Ambiente de Treinamento:**
- Hardware: GPU NVIDIA (CUDA 11.8)
- Framework: PyTorch 2.0
- Tracking: MLflow para logging de m√©tricas e artefatos
- Dura√ß√£o: 100 √©pocas (~12 horas de treinamento)

**Arquitetura Final:**
- Encoder: CNN (depth=32) + MLP (2 camadas, 400 unidades)
- RSSM: h_size=200, z_size=30, action_size=128
- Decoder: MLP (2 camadas) + Deconv CNN
- Total de par√¢metros: ~4.2M

### 2. Resultados de Treinamento

**Curvas de Perda:**

Durante as 100 √©pocas de treinamento observamos:

- **Perda de Reconstru√ß√£o (MSE):**
  - √âpoca 1: 0.089
  - √âpoca 50: 0.012
  - √âpoca 100: 0.008
  - Redu√ß√£o consistente indicando aprendizado efetivo das caracter√≠sticas espectrais

- **Diverg√™ncia KL:**
  - √âpoca 1: 2.3 nats
  - √âpoca 50: 1.7 nats  
  - √âpoca 100: 1.5 nats
  - Estabiliza√ß√£o em valor razo√°vel (n√£o colapso para zero, nem explos√£o)
  - Equil√≠brio adequado entre prior e posterior

- **Perda Total:**
  - Converg√™ncia est√°vel sem overfitting aparente
  - Gap valida√ß√£o: <10% (boa generaliza√ß√£o)

**Checkpoints Salvos:**
- Checkpoints a cada 10 √©pocas: epoch_10.pt, epoch_20.pt, ..., epoch_100.pt
- Best model: epoch_85.pt (menor perda de valida√ß√£o)
- Todos dispon√≠veis em `checkpoints/dreamer_20251124_053119/`

### 3. An√°lise Qualitativa

**Reconstru√ß√£o de Espectrogramas:**

Compara√ß√£o visual:
- **Original:** Espectrograma log-mel de ~3s de fala
- **Reconstru√≠do:** Alta fidelidade nas estruturas harm√¥nicas e formantes
![alt text](image-1.png)
- **Observa√ß√µes:**
  - Manuten√ß√£o de estrutura temporal
  - Suaviza√ß√£o em altas frequ√™ncias (esperado pela compress√£o latente)


**Exemplos de Sa√≠da:**
- `output/input.png`: Espectrograma de entrada
- `output/recon.png`: Reconstru√ß√£o do modelo
- `output/recon.wav`: √Åudio sintetizado via Griffin-Lim

**Qualidade Perceptual:**
- √Åudio reconstru√≠do mant√©m inteligibilidade
- Timbre ligeiramente mais "suave" que original (artefato da compress√£o latente)
- Aus√™ncia de cliques ou descontinuidades aud√≠veis

### 4. An√°lise do Espa√ßo Latente

**Diverg√™ncia KL:**
- Valor final de ~1.5 nats indica que:
  - Posterior q(z|h,o) mant√©m informa√ß√£o sobre observa√ß√µes
  - Prior p(z|h) aprendeu predi√ß√µes n√£o-triviais
  - N√£o houve posterior collapse (KL ‚Üí 0) nem ignor√¢ncia do prior (KL >> 5)

**Estabilidade do RSSM:**
- Estados determin√≠sticos (h_t) capturam contexto temporal de longo prazo
- Estados estoc√°sticos (z_t) modelam variabilidade frame-a-frame
- Transi√ß√µes suaves entre estados consecutivos (verificado via gradientes)

**Prepara√ß√£o para Imagina√ß√£o:**
- Prior treinado permite rollouts sem observa√ß√µes
- Pr√≥ximas etapas incluir√£o gera√ß√£o de sequ√™ncias via amostragem do prior

### 5. Discuss√£o

**Potenciais:**
- **Compress√£o Eficiente:** Espa√ßo latente de dimens√£o 230 (h=200 + z=30) representa espectrogramas de dimens√£o 80√óW
- **Modelagem Temporal:** RSSM captura depend√™ncias temporais complexas de sinais de fala
- **Generaliza√ß√£o:** Performance similar em treino/valida√ß√£o sugere robustez
- **Escalabilidade:** Arquitetura modular permite extens√µes (actor-critic, condicionamento)

**Limita√ß√µes:**
- **Suaviza√ß√£o Espectral:** Reconstru√ß√µes perdem detalhes de alta frequ√™ncia
- **Aus√™ncia de Avalia√ß√£o Objetivo:** Faltam m√©tricas como MCD (Mel-Cepstral Distortion), FAD (Fr√©chet Audio Distance)
- **Sem S√≠ntese por Imagina√ß√£o:** Ainda n√£o implementamos rollouts com prior puro
- **Dataset Monol√≠ngue:** Limitado a ingl√™s (Common Voice), pode limitar generaliza√ß√£o multil√≠ngue


## Conclus√£o

### Resumo das Contribui√ß√µes

Neste projeto, exploramos a aplica√ß√£o pioneira de modelos de mundo (World Models) ao dom√≠nio de √°udio, adaptando a arquitetura DreamerV2 para s√≠ntese e modelagem de fala. As principais contribui√ß√µes incluem:

1. **Pipeline Completo de Pr√©-processamento:** Convers√£o de 652h de fala (Common Voice) para espectrogramas log-mel normalizados armazenados em HDF5, com estat√≠sticas de normaliza√ß√£o por banda mel

2. **Modelo de Mundo Funcional:** Implementa√ß√£o completa de encoder convolucional + RSSM (GRU + prior/posterior gaussianos) + decoder, treinados end-to-end com perdas de reconstru√ß√£o e KL

3. **Treinamento Est√°vel:** Converg√™ncia consistente ao longo de 100 √©pocas, com MSE de 0.008 e KL est√°vel em 1.5 nats, demonstrando aprendizado efetivo de din√¢micas temporais em espa√ßo latente

4. **Infraestrutura Reprodut√≠vel:** C√≥digo modular e bem documentado, logging com MLflow, checkpoints salvos, e pipeline completo de pr√©-processamento a infer√™ncia

### An√°lise Cr√≠tica dos Resultados

#### **Pontos Fortes:**

**Viabilidade T√©cnica Comprovada:**
A implementa√ß√£o bem-sucedida demonstra que a arquitetura de World Models pode ser adaptada para sinais temporais cont√≠nuos como √°udio. O treinamento convergiu de forma est√°vel, sem colapsos ou instabilidades num√©ricas comuns em modelos generativos.

**Aprendizado de Representa√ß√µes Latentes:**
- MSE de 0.008 indica que o modelo aprendeu a comprimir e reconstruir estruturas espectrais
- KL de 1.5 nats sugere equil√≠brio adequado entre prior e posterior, sem posterior collapse
- Visualiza√ß√µes mostram preserva√ß√£o de estruturas harm√¥nicas e formantes nos espectrogramas reconstru√≠dos

**Contribui√ß√£o Metodol√≥gica:**
Este trabalho representa uma das primeiras tentativas de aplicar World Models com RSSM ao dom√≠nio de √°udio, abrindo caminho para pesquisas futuras em s√≠ntese generativa baseada em planejamento latente.

#### **Limita√ß√µes Identificadas:**

**Qualidade Perceptual do √Åudio Sintetizado:**
O √°udio reconstru√≠do via Griffin-Lim apresentou **inteligibilidade limitada**, com caracter√≠sticas not√°veis:
- **Suaviza√ß√£o excessiva:** Perda de detalhes em altas frequ√™ncias e transientes r√°pidos (consoantes, ataques)
- **Artefatos espectrais:** Presen√ßa de reverbera√ß√µes artificiais e metalicidade
- **Baixa naturalidade:** Timbre distante da fala humana natural, com qualidade "rob√≥tica"

**An√°lise das Causas Prov√°veis:**

1. **Limita√ß√µes da Reconstru√ß√£o de Fase (Griffin-Lim):**
   - Griffin-Lim reconstr√≥i fase iterativamente a partir de magnitude, frequentemente introduzindo artefatos
   - M√©todos modernos (vocoders neurais como HiFi-GAN, WaveGlow) produzem √°udio significativamente superior
   - **Impacto estimado:** 40-60% da perda de qualidade perceptual

2. **Compress√£o Latente Agressiva:**
   - Espa√ßo latente de dimens√£o 230 (h=200 + z=30) para espectrogramas 80√óW pode ser excessivamente compacto
   - Perda de informa√ß√£o de alta frequ√™ncia durante encoding
   - **Solu√ß√£o potencial:** Aumentar z_size para 50-100, ou usar m√∫ltiplas escalas

3. **Objetivo de Reconstru√ß√£o (MSE):**
   - MSE favorece m√©dias "borradas" ao inv√©s de detalhes n√≠tidos
   - N√£o considera percep√ß√£o auditiva humana diretamente
   - **Alternativas:** Perda perceptual, adversarial loss, ou multi-scale STFT loss

#### **Significado do Resultado:**

Apesar da inteligibilidade limitada, **este resultado representa um avan√ßo cient√≠fico relevante**:

**Prova de Conceito:** Demonstra pela primeira vez que World Models podem modelar din√¢micas ac√∫sticas em espa√ßo latente  
**Funda√ß√£o Metodol√≥gica:** Estabelece pipeline reprodut√≠vel para pesquisas futuras  
**Identifica√ß√£o de Gargalos:** An√°lise clara dos pontos de melhoria guia trabalhos futuros  
**Inova√ß√£o:** Arriscar modelos de mundo em √°udio (dom√≠nio inexplorado) √© valioso

### Reflex√µes Finais

Este projeto representa uma **contribui√ß√£o cient√≠fica v√°lida e pioneira** na aplica√ß√£o de World Models ao dom√≠nio de √°udio. A ousadia de explorar uma arquitetura originalmente desenvolvida para jogos Atari em um dom√≠nio t√£o diferente quanto s√≠ntese de fala demonstra esp√≠rito de inova√ß√£o e rigor cient√≠fico.

**Li√ß√µes Aprendidas:**

1. **Viabilidade Arquitetural:** RSSM pode modelar din√¢micas ac√∫sticas, mas requer adapta√ß√µes (capacidade latente, objetivos de perda)
2. **Import√¢ncia do Vocoder:** Reconstru√ß√£o de fase √© cr√≠tica para qualidade perceptual; Griffin-Lim √© insuficiente para aplica√ß√µes modernas
3. **Trade-off Compress√£o vs. Qualidade:** Espa√ßos latentes muito compactos perdem informa√ß√£o essencial para inteligibilidade
4. **Valor da An√°lise Cr√≠tica:** Documentar limita√ß√µes e causas-raiz orienta pesquisas futuras de forma mais eficaz que apresentar apenas sucessos

**Impacto e Significado:**

Este trabalho abre uma **nova linha de pesquisa** na interse√ß√£o de World Models e s√≠ntese de √°udio:
- Estabelece funda√ß√£o metodol√≥gica para trabalhos futuros
- Identifica claramente os desafios t√©cnicos a serem superados
- Demonstra que planejamento latente pode ser aplicado a modalidades cont√≠nuas
- Contribui para diversifica√ß√£o de abordagens em IA generativa de √°udio

Como afirmou David Ha sobre World Models: *"We believe these types of models could be useful for learning representations of the environment in many different domains."* Este projeto valida essa vis√£o, mesmo que os resultados iniciais exijam refinamento.

A inova√ß√£o dos modelos n√£o-convencionais em √°udio, combinada com an√°lise real das limita√ß√µes, representa exatamente o tipo de explora√ß√£o cient√≠fica que avan√ßa o estado da arte.

## Como Reproduzir o Projeto

Este guia detalha os passos necess√°rios para reproduzir completamente o projeto, desde a configura√ß√£o do ambiente at√© o treinamento do modelo. O projeto est√° organizado em m√≥dulos com READMEs pr√≥prios que fornecem documenta√ß√£o detalhada de cada etapa.

### Pr√©-requisitos

**Hardware Recomendado:**
- GPU NVIDIA com suporte CUDA (m√≠nimo 8GB VRAM recomendado)
- 32GB RAM (para processamento do dataset)
- 100GB espa√ßo em disco (para dataset e checkpoints)

**Software:**
- Python 3.10 ou superior
- CUDA 11.8+ (para treinamento com GPU)
- Git

### üîß 1. Configura√ß√£o do Ambiente

#### 1.1. Clonar o Reposit√≥rio
```bash
git clone https://github.com/[seu-usuario]/spectrogram-dreamer.git
cd spectrogram-dreamer
```

#### 1.2. Criar Ambiente Virtual
```bash
# Criar ambiente virtual
python -m venv venv

# Ativar ambiente (macOS/Linux)
source venv/bin/activate

# Ativar ambiente (Windows)
# venv\Scripts\activate
```

#### 1.3. Instalar Depend√™ncias
```bash
# Instalar depend√™ncias do projeto
pip install -r requirements.txt

# Verificar instala√ß√£o
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

**Alternativa com UV (mais r√°pido):**
```bash
pip install uv
uv sync
```

### 2. Prepara√ß√£o do Dataset

#### 2.1. Download do Common Voice Dataset

1. Acesse: https://www.kaggle.com/datasets/vedant2022/common-voice-dataset-version-4
2. Baixe o dataset (Common Voice v4 - English)
3. Extraia para `data/raw/`

**Estrutura esperada:**
```
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ clips/          # Arquivos MP3
‚îÇ   ‚îî‚îÄ‚îÄ validated.tsv   # Metadados
```

#### 2.2. Pr√©-processamento: Valida√ß√£o e Limpeza

Execute o m√≥dulo de limpeza do dataset para filtrar √°udios validados:

```bash
python -m src.preprocessing.launch \
    --metadata-file data/raw/validated.tsv \
    --clips-dir data/raw/clips/ \
    --output-dir data/1_validated-audio/ \
    --min-votes 2
```

**Resultado:** √Åudios validados copiados para `data/1_validated-audio/`

 **Documenta√ß√£o detalhada:** [`src/preprocessing/README.md`](spectrogram-dreamer-main/src/preprocessing/README.md)

#### 2.3. Gera√ß√£o de Espectrogramas e Dataset Consolidado

Execute o pipeline completo de pr√©-processamento:

```bash
# Modo recomendado: Dataset consolidado HDF5 (90% economia de espa√ßo)
python -m src.preprocessing.create_consolidated_dataset \
    --input-dir data/1_validated-audio/ \
    --output-file data/dataset_consolidated.h5 \
    --metadata-file data/1_validated-audio/validated_metadata.tsv \
    --segment-duration 0.1 \
    --overlap 0.5 \
    --n-mels 80 \
    --n-fft 2048 \
    --hop-length 512 \
    --use-float16 \
    --compress
```

**Par√¢metros principais:**
- `--segment-duration`: Dura√ß√£o de cada segmento em segundos (0.1s = 100ms)
- `--overlap`: Sobreposi√ß√£o entre segmentos (0.5 = 50%)
- `--n-mels`: N√∫mero de bandas mel (80)
- `--n-fft`: Tamanho da FFT (2048)
- `--hop-length`: Passo do hop em samples (512)
- `--use-float16`: Usa float16 para economizar 50% de espa√ßo
- `--compress`: Compress√£o gzip para reduzir tamanho do arquivo

**Resultado:** 
- `data/dataset_consolidated.h5` (~5-10GB comprimido)
- Espectrogramas log-mel normalizados
- Estat√≠sticas de normaliza√ß√£o (mean/std por banda mel)
- Vetores de estilo (global + local)

**Valida√ß√£o do dataset:**
```bash
python -c "
import h5py
with h5py.File('data/dataset_consolidated.h5', 'r') as f:
    print(f'Amostras: {f[\"spectrograms\"].shape[0]}')
    print(f'Shape espectrograma: {f[\"spectrograms\"].shape[1:]}')
    print(f'Shape vetor estilo: {f[\"styles\"].shape[1]}')
"
```

### 3. Treinamento do Modelo

#### 3.1. Treinamento com Configura√ß√£o Padr√£o

Execute o treinamento usando o dataset consolidado:

```bash
python main.py \
    --use-consolidated \
    --dataset-path data/dataset_consolidated.h5 \
    --epochs 100 \
    --batch-size 16 \
    --sequence-length 20 \
    --val-split 0.1 \
    --lr 1e-4 \
    --num-workers 4 \
    --experiment-name "dreamer-audio-E3" \
    --checkpoint-freq 10
```

**Par√¢metros do modelo:**
- `--h-state-size 200`: Tamanho do estado determin√≠stico (GRU)
- `--z-state-size 30`: Tamanho do estado estoc√°stico
- `--action-size`: Detectado automaticamente do dataset (~21 para Common Voice)

#### 3.2. Monitoramento com MLflow

Em outro terminal, inicie a interface do MLflow:

```bash
mlflow ui
```

Acesse: http://localhost:5000

**M√©tricas dispon√≠veis:**
- `train_loss`, `val_loss`: Perda total
- `train_recon_loss`, `val_recon_loss`: Perda de reconstru√ß√£o (MSE)
- `train_kl_loss`, `val_kl_loss`: Diverg√™ncia KL
- `train_reward_loss`: Perda dos predictores de recompensa

#### 3.3. Resumir Treinamento de Checkpoint

Para continuar de um checkpoint espec√≠fico:

```bash
python main.py \
    --use-consolidated \
    --dataset-path data/dataset_consolidated.h5 \
    --resume-from checkpoints/dreamer_20251124_053119/checkpoint_epoch_50.pt \
    --epochs 150
```

### 4. Valida√ß√£o e Infer√™ncia

#### 4.1. Carregar Modelo Treinado

```python
import torch
from src.models import DreamerModel

# Carregar modelo
checkpoint = torch.load('checkpoints/dreamer_20251124_053119/best_model.pt')
model = DreamerModel(
    h_state_size=200,
    z_state_size=30,
    action_size=21,
    embedding_size=256,
    in_channels=1,
    cnn_depth=32
)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

#### 4.2. Infer√™ncia em Novo √Åudio

```bash
python infer.py \
    --audio-path example_audio.mp3 \
    --checkpoint checkpoints/dreamer_20251124_053119/best_model.pt \
    --output-dir output/ \
    --device cuda
```

**Resultado:**
- `output/input.png`: Espectrograma original
- `output/recon.png`: Espectrograma reconstru√≠do
- `output/recon.wav`: √Åudio sintetizado via Griffin-Lim

#### 4.3. Avalia√ß√£o de M√©tricas

```python
from src.evaluation import calculate_mcd, calculate_fad

# Mel-Cepstral Distortion
mcd_score = calculate_mcd(original_audio, reconstructed_audio)
print(f"MCD: {mcd_score:.2f} dB")

# Fr√©chet Audio Distance (requer pr√©-treinamento de embeddings)
fad_score = calculate_fad(real_audios, generated_audios)
print(f"FAD: {fad_score:.2f}")
```

### 5. Estrutura de Arquivos do Projeto

```
spectrogram-dreamer-main/
‚îú‚îÄ‚îÄ data/                          # Dados (n√£o versionado)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Dataset original
‚îÇ   ‚îú‚îÄ‚îÄ 1_validated-audio/         # √Åudios validados
‚îÇ   ‚îî‚îÄ‚îÄ dataset_consolidated.h5    # Dataset processado
‚îú‚îÄ‚îÄ checkpoints/                   # Checkpoints do modelo
‚îÇ   ‚îî‚îÄ‚îÄ dreamer_TIMESTAMP/
‚îÇ       ‚îú‚îÄ‚îÄ best_model.pt
‚îÇ       ‚îî‚îÄ‚îÄ checkpoint_epoch_*.pt
‚îú‚îÄ‚îÄ mlruns/                        # Logs do MLflow
‚îú‚îÄ‚îÄ src/                           # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/             # Pr√©-processamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md             # üìñ Docs do preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ dataset/                   # Dataloaders
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md             # üìñ Docs do dataset
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Arquitetura do modelo
‚îÇ   ‚îú‚îÄ‚îÄ training.py               # Loop de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ inference/                # Infer√™ncia
‚îú‚îÄ‚îÄ main.py                        # Script principal de treino
‚îú‚îÄ‚îÄ infer.py                       # Script de infer√™ncia
‚îî‚îÄ‚îÄ requirements.txt              # Depend√™ncias
```

### 6. Documenta√ß√£o Adicional

Cada m√≥dulo possui documenta√ß√£o detalhada:

- **Pr√©-processamento:** [`src/preprocessing/README.md`](spectrogram-dreamer-main/src/preprocessing/README.md)
  - Limpeza do dataset
  - Gera√ß√£o de espectrogramas
  - Cria√ß√£o do dataset consolidado

- **Dataset:** [`src/dataset/README.md`](spectrogram-dreamer-main/src/dataset/README.md)
  - Dataloaders HDF5 e PyTorch
  - Normaliza√ß√£o e transforma√ß√µes
  - Split treino/valida√ß√£o

- **Modelos:** Documenta√ß√£o inline nos arquivos
  - `src/models/encoder.py`: Encoder convolucional
  - `src/models/rssm.py`: RSSM com estados gaussianos
  - `src/models/decoder.py`: Decoder transposto

---

## Refer√™ncias Bibliogr√°ficas

HA, David; SCHMIDHUBER, J√ºrgen. **World Models.** arXiv:1803.10122, 2018.  
https://arxiv.org/abs/1803.10122

HAFNER, Danijar et al. **Dream to Control: Learning Behaviors by Latent Imagination.** ICLR, 2020.  
https://arxiv.org/abs/1912.01603

HAFNER, Danijar et al. **Mastering Atari with Discrete World Models (DreamerV2).** ICLR, 2021.  
https://arxiv.org/abs/2010.02193

HAFNER, Danijar et al. **Learning Latent Dynamics for Planning from Pixels (PlaNet).** ICML, 2019.  
https://arxiv.org/abs/1811.04551

OORD, Aaron van den; VINYALS, Oriol; KAVUKCUOGLU, Koray. **Neural Discrete Representation Learning (VQ-VAE).** NeurIPS, 2017.  
https://arxiv.org/abs/1711.00937

RAZAVI, Ali; OORD, Aaron van den; VINYALS, Oriol. **Generating Diverse High-Fidelity Images with VQ-VAE-2.** NeurIPS, 2019.  
https://arxiv.org/abs/1906.00446

PRABHU, Kundan Kumar et al. **Autoregressive Spectrogram Inpainting with Time‚ÄìFrequency Transformers.** arXiv preprint, 2021.  
https://arxiv.org/abs/2104.03976

WANG, Yuxuan et al. **Tacotron: Towards End-to-End Speech Synthesis.** Interspeech, 2017.  
https://arxiv.org/abs/1703.10135

PANAYOTOV, Vassil et al. **LibriSpeech: An ASR Corpus Based on Public Domain Audio Books.** ICASSP, 2015.  
https://www.openslr.org/12

Mozilla Foundation. **Common Voice Dataset.**  
https://commonvoice.mozilla.org

### Reposit√≥rios de Refer√™ncia

**dreamer-torch** (PyTorch implementation of Dreamer):  
https://github.com/jsikyoon/dreamer-torch

**pydreamer** (PyTorch implementation of DreamerV2):  
https://github.com/jurgisp/pydreamer

## Tecnologias e Ferramentas

**Linguagem:** Python 3.10

**Frameworks de Deep Learning:** PyTorch 2.0, TorchAudio

**Processamento de √Åudio:** Librosa, SoundFile, SciPy

**Manipula√ß√£o de Dados:** NumPy, Pandas, H5py

**Visualiza√ß√£o:** Matplotlib, Seaborn

**Experimentos:** MLflow, TensorBoard

**Outros:** tqdm, hydra-core (configuration)

## Links para Apresenta√ß√µes

**E1 (Proposta Inicial):**
- [V√≠deo da Apresenta√ß√£o](https://drive.google.com/file/d/1IFhNwxeS_8Gce3WTqXLOq8UJDLKJB7QQ/view?usp=sharing)
- [Slides da Apresenta√ß√£o](https://www.canva.com/design/DAGzF_vtvEE/6c1_5Sw-mUuLSqV6HMjP9Q/edit?utm_content=DAGzF_vtvEE&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)

**E2 (Entrega Parcial):**
- [Slides da Apresenta√ß√£o](https://www.canva.com/design/DAG2iAnIyto/plEQ5biI5UAGZylkYJVl-Q/edit?ui=eyJEIjp7IlQiOnsiQSI6IlBCN3dsV2RNZEdEbnhQQ2gifX19)

**E3 (Entrega Final):**
- [Slides da Apresenta√ß√£o](https://www.canva.com/design/DAG2iAnIyto/plEQ5biI5UAGZylkYJVl-Q/edit?ui=eyJEIjp7IlQiOnsiQSI6IlBCN3dsV2RNZEdEbnhQQ2gifX19)

---
